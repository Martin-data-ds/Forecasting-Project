Using Time-Series Forecasting to Forecast Future Bakery Sales
Time-series analysis involves an analysis of any variable that changes with time. This analysis is typically used to show how a particular phenomenon reacts as time changes. Variables respond differently with time, with some of them following an increasing or a decreasing trend, some could show seasonality, peaking during specific periods, and dropping in certain periods. Some variables also assume a random movement with no apparent seasonality or trend. It’s also possible to have seasons that follow a trend. With this information, it is possible to forecast future values by using the behavior of the past values or data.
Time series forecasting entails building models that are able to first learn from the data and make predictions regarding the future movements of the data. Time series forecasting can be applied across multiple fields; for example, in this project, I used a dataset relating to the sales of a bakery store. The task at hand was to use Python to build an effective model that is able to forecast the number of units of a particular product that the store will sell in the future.
I built and tested some baseline models that relate to time series forecasting. They include: the Naïve model, the Historic Average, the Window Average, and the Seasonal Naïve model. With the help of the StatsForecast library, I was able to run all these models at the same time, saving lots of time. There was a noted increase in performance of all the models when I increased the number of days to make the forecast. 21-day forecasts performed better than 7-day or 14-day forecasts generally. By analyzing the Mean Absolute Error (MAE) together with the Mean Squared Error (MSE) on the performance of each model, the seasonal Naïve model, run with a weekly season length, proved to be the best performer with the least error, suggesting that there was a strong weekly seasonality effect in the sales of the products in the bakery.
I also built more advanced statistical models: the ARIMA and SARIMA, and compared the performance. The seasonal Naïve model persisted as the best model, and SARIMA closely followed with a slightly higher test score. However, rerunning the three models through cross-validation, SARIMA became the best forecaster, followed by ARIMA, and the Seasonal Naïve now came last.
Considering exogenous variables of our dataset, including the unit price column earlier excluded in training, and the exogenous time variable that I made through feature engineering, I built two different SARIMA models, each now with these exogenous variables. The new models with the exogenous variables, however, had a weaker performance compared to the original SARIMA. In this case, SARIMA stood out as the best model for this dataset. If any forecast was to be made in order to make decisions and to plan for the future sales of the bakery, it would be to run the SARIMA model that had been passed through cross-validation.
In the notebook provided below, I included the data cleaning process, which narrowed our data by excluding products that had very minimal records of sales. I also had the opportunity to present the codes used and visuals that show the forecasts of the sales of some chosen products of each of our models, and compare that to the past sales to show if the models clearly learnt the patterns. Visuals of the performances of our models were also included.
When building the forecasting models, I learnt that:
•	The utilsforecast library provides us with plot_series and evaluate, modules that are used to plot time series and evaluate our models, respectively.
•	Passing our models through cross-validation makes our models more reliable than just running the models by themselves.
•	Statistical models are expected to perform better than the baseline models.
•	By increasing the number of days to forecast, the models performed slightly better than with fewer days to forecast.
•	We can forecast by considering other dependent variables or not. Our model performed better when not considering the exogenous variables.
•	Other forecast error metrics, including the MAPE (Mean Absolute Percentage Error), MASE (Mean Absolute Scaled Error), and sMAPE (Symmetric Mean Absolute Percentage Error), can be effective to evaluate the performance of our forecasts.
